# ============================================================================
# Deep Search AI Assistant -- Global Settings
# ============================================================================
# This file is the single source of truth for every path, parameter, and
# threshold used across the pipeline.  Modules should read from here rather
# than hard-coding values so that experiments are reproducible and changes
# propagate consistently.
# ============================================================================

# -- Paths -------------------------------------------------------------------
paths:
  # Raw datasets (READ-ONLY -- never write here).
  raw_data_root: "/mnt/d/Openvino-project/data/raw"

  # All pipeline outputs land under this directory.
  processed_root: "data/processed"

  # Sub-directories created automatically by each pipeline stage.
  normalised_dir: "data/processed/normalised"
  chunks_dir: "data/processed/chunks"
  embeddings_dir: "data/processed/embeddings"
  faiss_dir: "data/processed/faiss"
  ocr_cache_dir: "data/processed/ocr_cache"

  # Runtime logs.
  log_dir: "logs"

# -- Datasets ----------------------------------------------------------------
# Each entry maps a short CLI name to its location under raw_data_root.
# "format" is either "dataset_dict" (has splits/) or "dataset" (flat arrow).
datasets:
  funsd:
    path: "funsd"
    format: "dataset_dict"
    splits: ["train", "test"]
    description: "FUNSD form understanding -- words, bboxes, NER tags, images"

  docvqa:
    path: "docvqa"
    format: "dataset"
    description: "DocVQA subset -- question/answer pairs over document images"

  rvl_cdip:
    path: "rvl_cdip"
    format: "dataset"
    description: "RVL-CDIP subset -- 16-class document image classification"

# -- OCR ---------------------------------------------------------------------
ocr:
  engine: "tesseract"            # "tesseract" or "paddleocr"
  tesseract_lang: "eng"
  paddleocr_lang: "en"           # PaddleOCR language ("en", "ch", etc.)
  paddleocr_use_openvino: false  # Use OpenVINO backend for PaddleOCR
  preprocess: true               # apply adaptive-threshold before OCR (Tesseract only)
  confidence_threshold: 40       # discard words below this confidence (0-100)

# -- Chunking ----------------------------------------------------------------
chunking:
  strategy: "fixed"              # "fixed" character-window; "sentence" planned
  chunk_size: 512                # characters per chunk
  chunk_overlap: 64              # overlap between consecutive chunks
  min_chunk_length: 30           # discard chunks shorter than this

# -- Embeddings --------------------------------------------------------------
embeddings:
  model_name: "sentence-transformers/all-MiniLM-L6-v2"
  dimension: 384
  batch_size: 64
  device: "cpu"                  # "cpu" for stability; "cuda" if VRAM permits
  normalise: true                # L2-normalise before indexing

# -- FAISS -------------------------------------------------------------------
faiss:
  index_type: "IndexFlatIP"      # inner-product on L2-normed vectors = cosine
  nprobe: 10                     # only relevant if you switch to IVF later

# -- Retrieval ---------------------------------------------------------------
retrieval:
  top_k: 5                       # number of chunks returned per query
  score_threshold: 0.0           # minimum similarity to include in results

# -- LLM (Ollama) ------------------------------------------------------------
llm:
  provider: "openvino"           # "ollama" or "openvino"
  model: "mistral"               # Ollama: model name; OpenVINO: ignored (uses openvino.llm_model_dir)
  endpoint: "http://localhost:11434"
  temperature: 0.3
  max_tokens: 512
  timeout_seconds: 120

# -- OpenVINO ----------------------------------------------------------------
# Set enabled: true to use OpenVINO for embedding inference instead of
# PyTorch/sentence-transformers.  The pipeline reads these settings via
# DeviceManager.select_from_settings().
openvino:
  enabled: true                  # true = use OV encoder, false = use PyTorch
  device: "CPU"                  # "CPU", "GPU", "NPU", "AUTO", "MULTI:CPU,GPU"
  embedding_model_ir: "models/ov/all-MiniLM-L6-v2/model.xml"
  llm_model_dir: "models/ov/mistral-7b-instruct"  # path to converted LLM directory

# -- Video Ingestion -------------------------------------------------------
# Settings for the video extraction and interpretation pipeline.
# These are used by `python cli.py ingest-videos`.
video:
  # Dataset name: "msrvtt" or "custom".  Controls how files are discovered.
  dataset_name: "msrvtt"

  # Path to raw video dataset root (overridable via CLI --video-root).
  # MSR-VTT layout:  <root>/annotation/MSR_VTT.json  +  <root>/videos/all/*.mp4
  video_data_root: "/mnt/d/Openvino-project/data/raw/archive/data/MSRVTT/MSRVTT"

  # Enable Whisper transcription.  Set to false for caption-based datasets
  # like MSR-VTT where captions are already provided in the annotation JSON.
  enable_whisper: false

  # Extract 1 frame every N seconds (for frame sampling + OCR).
  frame_interval: 5

  # Whisper model variant: "tiny", "base", "small", "medium", "large".
  whisper_model_size: "small"

  # Device for Whisper inference: "cpu" or "cuda".
  whisper_device: "cpu"

  # Enable/disable OCR on sampled frames.
  enable_frame_ocr: true

  # Minimum word count for a frame's OCR output to be kept.
  ocr_min_words: 3

  # Duration of each temporal chunk in seconds (transcript + OCR merge window).
  chunk_interval: 30

  # Processed output directory (relative to project root).
  output_dir: "data/processed/videos"

